---
title: "HighDimensionCluster"
author: "Jinchang Fan"
date: "3/24/2019"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = F)
library(tidyverse)
library(reshape2)
library(shallot)
library(cowplot)
```

#1. Data simulation

Simulate high-dimensional data (p=1000) with three groups of observations where the number of observations is n=100

```{r}
set.seed(0)
n_rows = 1000
n_cols = 100
n_genes = 1000
n_cells = 100

k=3
x_mus = c(0,5,5)
x_sds = c(1,0.1,1)
y_mus = c(5,5,0)
y_sds = c(1,0.1,1)
prop1 = c(0.3,0.5,0.2)

comp1 <- sample(seq_len(k), prob=prop1, size=n_cols, replace=TRUE)
samples1 <- cbind(rnorm(n=n_cols, mean=x_mus[comp1],sd=x_sds[comp1]),
                  rnorm(n=n_cols, mean=y_mus[comp1],sd=y_sds[comp1]))

proj <- matrix(rnorm(n_rows* n_cols), nrow=n_rows, ncol=2)
A1 <- samples1 %*% t(proj)
A1 <- A1 + rnorm(n_genes*n_cells)
```

#2. Perform k-means to identify the number of clusters in the data. 

##kmeans after PCA

The structure of high dimension data is more complicated, so try dimension reduction at first, here Perfrom principal component analysis. Considering the large size of correlation matrix, it is more efficient to perfrom singular value decomposition as the singular value is the square root of eigne value. The below figure shows the explained variance of every single component, it is clear the the first two components contain much more information than others, and the remaining are all at the same level. In that case the first two components could be picked to perform clustering.

```{r}
mysvd <- svd(A1)
qplot(x = c(1:100), y= mysvd$d/sum(mysvd$d) )+geom_line()+geom_point()+
  labs(title='explained variance of componets', x='components',y='explained variance of single component')
```



```{r}
A1_pc <- A1 %*% mysvd$v[,1:2]
qplot(A1_pc[,1],A1_pc[,2])+labs(title ='scatter plot of first two components',x='component 1', y='compoment 2')
```

Based on the scatter plot we can intuitively guess there are three groups, but more regorious analysis is required. 


```{r}
SSW <- rep(NA,7) 
for(i in 1:7){
  SSW[i] <- kmeans(A1_pc,i)$tot.withinss
}
qplot(c(1:7),SSW)+geom_line()+geom_point()+
  labs(title='Within Cluster Sum of Squares for optimal number of clusters', 
       x='number of clusters',y='Within Cluster Sum of Squares')
```

Concerning the Within Cluster Sum of Squares, the information could not explained by clusters decreases rapidly when k increases from 1 to 3, and goes flat with further increasing. Thus the optimal number of cluster should be 3.


```{r}
AIC <- rep(NA,7)
BIC <- rep(NA,7)
for(i in 1:7){
  km_temp <- kmeans(A1_pc,i)
  ssw <- km_temp$tot.withinss
  m <- ncol(km_temp$centers)
  n <- length(km_temp$cluster)
  AIC[i] <- ssw + 2*m*i
  BIC[i] <- ssw + 2*n*m*i
}

plot_aic <- qplot(c(1:7),AIC)+geom_line()+geom_point()+
  labs(x='number of clusters',y='AIC')

plot_bic <- qplot(c(1:7),BIC)+geom_line()+geom_point()+
  labs(x='number of clusters',y='BIC')
plot_grid(plot_aic, plot_bic,nrow = 1, labels = c("AIC","BIC"))
```

Try to penish parameter by AIC and BIC. Figure above shows the AIC and BIC or different k values. It is quite similar compated with the result without panalty, as the scale of sum of square error is great larger than panalty on parameters. So minimize AIC or BIC is not a good idea to determine k.

Since AIC nor BIC is a good fit, elbow method is applied. In particular is to find position for maximum second derivative. Result is k=3 and the cluster result is as following:

```{r}
K <- which.max(diff(diff(SSW)))+1 #capital K to distinct with previous small k
km_pca <- kmeans(A1_pc,K)
center <- data.frame(pc1 = km_pca$centers[,1], pc2 = km_pca$centers[,2], label = as.factor(1:K))
data.frame(pc1 = A1_pc[,1], pc2 = A1_pc[,2], label = as.factor(km_pca$cluster)) %>% 
  ggplot( aes(pc1,pc2, color = label))+geom_point()+
  annotate('point', x = km_pca$centers[,1], y = km_pca$centers[,2], size = 5, alpha = 0.5)+
  labs(title ='clustered scatter plot of first two components',x='component 1', y='compoment 2')
```


##kmeans for original data

For comparison similar clustering for original data without pca is performed as well. It is hard to make clear data visualization on high dimension, so just Within Cluster Sum of Square, AIC and BIC are calculated.

Concerning the Within Cluster Sum of Squares, the result is quite familiar with clustering after pca. Information could not explained by clusters decreases rapidly when k increases from 1 to 3, and goes flat with further increasing of k. Thus the optimal number of cluster should be 3.

```{r}
SSW <- rep(NA,7) 
for(i in 1:7){
  SSW[i] <- kmeans(A1,i)$tot.withinss
}
qplot(c(1:7),SSW)+geom_line()+geom_point()+
  labs(title='Within cluster Sum of Squares for optimal number of clusters', 
       x='number of clusters',y='Within cluster Sum of Squares')
```

Penish parameter by AIC and BIC. BIC is found to have a sharp minimal value, as penalty is at the same scale as sum of squared error after multiplied by dimension of variables .  

```{r}
AIC <- rep(NA,7)
BIC <- rep(NA,7)
for(i in 1:7){
  km_temp <- kmeans(A1,i)
  ssw <- km_temp$tot.withinss
  m <- ncol(km_temp$centers)
  n <- length(km_temp$cluster)
  AIC[i] <- ssw + 2*m*i
  BIC[i] <- ssw + 2*n*m*i
}

plot_aic <- qplot(c(1:7),AIC)+geom_line()+geom_point()+
  labs(x='number of clusters',y='AIC')

plot_bic <- qplot(c(1:7),BIC)+geom_line()+geom_point()+
  labs(x='number of clusters',y='BIC')
plot_grid(plot_aic, plot_bic,nrow = 1, labels = c("AIC","BIC"))
```



```{r}
K <- which.min(BIC)
km <- kmeans(A1, K)
```


#3. To assess the accuracy, calculate the adjusted rand index and then calculate the within clusters sum of squares. 

adjusted rand index for kmeans after pca:

```{r}
adj.rand.index(comp1, km_pca$cluster)
```

within clusters sum of squares:

```{r}
#given data and their cluster label, calculate within clusters sum of squares(ss), between cluster ss and total ss. 
#input: X matrix n*p; label seq n
#output: 
SSE_cluster <- function(X,label){
  X_c <- apply(X, 2, function(x) x-mean(x))
  SST <- sum(diag(t(X_c) %*% X_c))
  SSW <- 0
  for(l in unique(label)){
    X_temp <- X[which(label==l),]
    X_tc <- apply(X_temp, 2, function(x) x-mean(x))
    SSW <- SSW + sum(diag(t(X_tc) %*% X_tc))
  }
  res <- matrix(c(SSW,SST,SSW/SST) ,ncol = 3)
  colnames(res) <- c('SSW','SST','Ratio')
  return(res)
}

SSE_cluster(A1, km_pca$cluster)
```


adjusted rand index for kmeans after pca:

```{r}
adj.rand.index(comp1, km$cluster)
```

within clusters sum of squares:

```{r}
SSE_cluster(A1, km$cluster)
```

In this perticular case identical cluster result is given, and adjusted rand index equals 1 which implies a perfect fit of the true cluster.


#Perform 100 times then compare and summary.:

```{r}
Trail <- function(){
  comp1 <- sample(seq_len(k), prob=prop1, size=n_cols, replace=TRUE)
  samples1 <- cbind(rnorm(n=n_cols, mean=x_mus[comp1],sd=x_sds[comp1]),
                    rnorm(n=n_cols, mean=y_mus[comp1],sd=y_sds[comp1]))
  
  proj <- matrix(rnorm(n_rows* n_cols), nrow=n_rows, ncol=2)
  A1 <- samples1 %*% t(proj)
  A1 <- A1 + rnorm(n_genes*n_cells)
  
  res_pca <- Kmean_pca(A1,comp1)
  colnames(res_pca) <- paste0(colnames(res_pca), '_pca')
  res_plain <- Kmean_plain(A1,comp1)
  colnames(res_plain) <- paste0(colnames(res_plain), '_plian')
  return(cbind(res_pca,res_plain))

}

Kmean_pca <- function(A1,comp1){
  time0 <- Sys.time()
  mysvd <- svd(A1)
  A1_pc <- A1 %*% mysvd$v[,1:2]
  SSW <- rep(NA,7)
  for(i in 1:7){
    km_temp <- kmeans(A1_pc,i)
    SSW[i] <- km_temp$tot.withinss
  }
  K <- which.max(diff(diff(SSW)))+1
  km_labels <- kmeans(A1_pc,K)$cluster
  time_ <- Sys.time() - time0
  SSE <- SSE_cluster(A1, km_labels)
  randind <- adj.rand.index(comp1, km_labels)
  res <- matrix(c(randind,SSE,time_),nrow = 1)
  colnames(res) <- c('randind','SSW','SST','Ratio','time')
  return(res)
}

Kmean_plain <- function(A1,comp1){
  time0 <- Sys.time()
  BIC <- rep(NA,7)
  for(i in 1:7){
    km_temp <- kmeans(A1,i)
    ssw <- km_temp$tot.withinss
    m <- ncol(km_temp$centers)
    n <- length(km_temp$cluster)
    BIC[i] <- ssw + 2*n*m*i
  }
  K <- which.min(BIC)
  
  km_labels <- kmeans(A1,K)$cluster
  time_ <- Sys.time() - time0
  SSE <- SSE_cluster(A1, km_labels)
  randind <- adj.rand.index(comp1, km_labels)
  res <- matrix(c(randind,SSE,time_),nrow = 1)
  colnames(res) <- c('randind','SSW','SST','Ratio','time')
  return(res)
}

t <- 100
res <- c()
for(i in 1:t){
  res <- rbind(res, Trail())
}
res <- data.frame(res)

```

Compare the adjusted rand index for two methods:

adjusted rand index for kmeans after pca:

```{r}
cat('mean: ',mean(res[,1]), 'standard diviation: ', sd(res[,1]))
```

adjusted rand index for simple kmeans:

```{r}
cat('mean: ',mean(res[,6]), 'standard diviation: ', sd(res[,6]))
```

within clusters sum of squares for kmeans after pca:

```{r}
cat('mean: ',mean(res[,4]), 'standard diviation: ', sd(res[,4]))
```

within clusters sum of squares for simple kmeans:

```{r}
cat('mean: ',mean(res[,9]), 'standard diviation: ', sd(res[,9]))
```

time efficiency of two method:

```{r}
cat('kmeans after pca: ',mean(res[,5]), 'simple kmeans: ', mean(res[,10]))
```

It shows that simple kmeans with determinating k by BIC shows better performance on adjusted rand index than kmeans after pca and determinante k by elbow (0.86 over 0.78), with similar standard diviation. While kmeans after pca is better on within clusterss sum of squares (0.33 over 0.26). And keams after pca has better time efficiency.

#Further discussion

 - Different methods to determinate k are applied to simple kmeans and kmeans after pca, As personally I think BIC would be a better method than elbow method. It is reasonable to use elbow for both if necessity of pca is considered.

 - Based on the scatter plot of first two pinciple components, the clusters have different variance, which violates the assumption of kmeans that clusters have constant variance. Method accepting different variance such as Gaussian mixture models could be considered for imporvement.
